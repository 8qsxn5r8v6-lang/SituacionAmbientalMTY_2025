{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Pipeline final adaptado a tus 7 archivos (Colab / Jupyter)\n",
        "# - No borra filas con NaN en 'value'\n",
        "# - Usa name-of-sheet como hint de estación cuando no detecta estación en headers\n",
        "# - Ignora columnas tipo \"<estacion>_b\"\n",
        "# - Normaliza abreviaturas (SE->SURESTE, CE->CENTRO, etc.)\n",
        "# - Guarda parquet y pivot excel al final\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# ------------------\n",
        "# Mapeos y canonicals\n",
        "# ------------------\n",
        "sheet_to_station = {\n",
        "    \"SE\": \"SURESTE\",\n",
        "    \"CE\": \"CENTRO\",\n",
        "    \"SO\": \"SUROESTE\",\n",
        "    \"NE2\": \"NOROESTE2\",\n",
        "    \"SE2\": \"SURESTE2\",\n",
        "    \"SE3\": \"SURESTE3\",\n",
        "    \"NE\": \"NORESTE\",\n",
        "    \"NO\": \"NOROESTE\",\n",
        "    \"NO2\": \"NOROESTE2\",\n",
        "    \"NTE\": \"NORTE\",\n",
        "    \"NTE2\": \"NORTE2\",\n",
        "    \"SO2\": \"SUROESTE2\",\n",
        "    \"SUR\": \"SUR\",\n",
        "    \"NO3\": \"NOROESTE3\",\n",
        "    \"NE3\": \"NORESTE3\"\n",
        "}\n",
        "sheet_to_station = {k.strip().upper(): v for k,v in sheet_to_station.items()}\n",
        "\n",
        "ESTACIONES_CANON = [\n",
        "    'SURESTE','NORESTE','CENTRO','NOROESTE','SUROESTE','NOROESTE2',\n",
        "    'NORTE','NORESTE2','SURESTE2','SUROESTE2','SURESTE3','SUR','NORTE2',\n",
        "    'NORESTE3','NOROESTE3'\n",
        "]\n",
        "\n",
        "CONTAMINANTES_CANON = ['CO','NO','NO2','NOX','O3','PM10','PM2.5','PRS','RAINF','RH','SO2','SR','TOUT','WSR','WDR']\n",
        "\n",
        "# ------------------\n",
        "# Utilidades generales\n",
        "# ------------------\n",
        "def hacer_unicos(cols):\n",
        "    vistos = {}\n",
        "    nuevas = []\n",
        "    for c in cols:\n",
        "        c = str(c)\n",
        "        if c not in vistos:\n",
        "            vistos[c] = 0\n",
        "            nuevas.append(c)\n",
        "        else:\n",
        "            vistos[c] += 1\n",
        "            nuevas.append(f\"{c}_{vistos[c]}\")\n",
        "    return nuevas\n",
        "\n",
        "def norm_text(x):\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    s = str(x).strip().upper()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.replace(\"\\u00A0\",\" \")\n",
        "    return s\n",
        "\n",
        "def best_match_token(token, candidates):\n",
        "    if token is None:\n",
        "        return None\n",
        "    t = norm_text(token)\n",
        "    if t == \"\":\n",
        "        return None\n",
        "    # exact\n",
        "    for c in candidates:\n",
        "        if t == norm_text(c):\n",
        "            return c\n",
        "    # substring\n",
        "    for c in candidates:\n",
        "        if norm_text(c) in t or t in norm_text(c):\n",
        "            return c\n",
        "    # token parts\n",
        "    parts = re.split(r'[\\s_\\-\\.]+', t)\n",
        "    for p in parts:\n",
        "        for c in candidates:\n",
        "            if p == norm_text(c) or norm_text(c) in p:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# elimina columnas tipo \"<estacion>_b\" (ignorarlas)\n",
        "def drop_b_columns(df):\n",
        "    cols = [c for c in df.columns if not re.search(r'(_b$)|(\\b_b\\b)|(^.*_B$)', str(c), flags=re.IGNORECASE)]\n",
        "    return df.loc[:, cols]\n",
        "\n",
        "# ------------------\n",
        "# Parsers por tipo de archivo (según tu descripción)\n",
        "# ------------------\n",
        "\n",
        "# 1) DATOS HISTÓRICOS 2020_Contaminante.xlsx\n",
        "# - cada hoja = contaminante\n",
        "# - primera col: (posiblemente) descriptor; segunda col: fecha/hora; luego estaciones (SE, NE, CE, NO, SO, NO2, Norte, NE2, SE2, SO2, SE3, Sur, NE3)\n",
        "# - hay columnas de estación '_b' al lado que IGNORAR\n",
        "def parse_2020_contaminante(path, sheet):\n",
        "    # leemos con header=0\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=0, engine='openpyxl')\n",
        "    # quitar columnas *_b\n",
        "    df = drop_b_columns(df)\n",
        "    # detectar la columna de fecha: buscar columna cuyo header contenga \"DATE\" o \"FECHA\" o \"TIME\" o \"HORA\"\n",
        "    cols_upper = [norm_text(c) for c in df.columns]\n",
        "    date_col = None\n",
        "    for c, cu in zip(df.columns, cols_upper):\n",
        "        if any(k in cu for k in ['FECHA','DATE','HORA','TIME','TIMESTAMP','DATETIME']):\n",
        "            date_col = c\n",
        "            break\n",
        "    # si no, considerar la segunda columna si la primera no parece fecha\n",
        "    if date_col is None:\n",
        "        if len(df.columns) >= 2:\n",
        "            date_col = df.columns[1]\n",
        "        else:\n",
        "            date_col = df.columns[0]\n",
        "    # convertir datetime\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True)\n",
        "    # las columnas estaciones son todas excepto la primera(s) (ignoramos la primera columna descriptiva)\n",
        "    # suponer que columnas con headers que coincidan con abreviaturas SE, NE, CE, NO, SO, etc. son estaciones\n",
        "    estacion_cols = []\n",
        "    for c in df.columns:\n",
        "        cu = norm_text(c)\n",
        "        # ignorar date col\n",
        "        if c == date_col:\n",
        "            continue\n",
        "        # if header indicates a station abbreviation\n",
        "        if any(abbr in cu for abbr in sheet_to_station.keys()) or best_match_token(cu, ESTACIONES_CANON):\n",
        "            estacion_cols.append(c)\n",
        "    # si no detectamos estaciones, asumimos todas las columnas después de date_col son estaciones\n",
        "    if not estacion_cols:\n",
        "        after_idx = list(df.columns).index(date_col) + 1\n",
        "        estacion_cols = list(df.columns)[after_idx:]\n",
        "    # melt: cada columna -> station/pollutant (sheet name)\n",
        "    rows = []\n",
        "    pollutant_hint = norm_text(sheet)  # sheet name is pollutant in this file\n",
        "    pollutant_guess = best_match_token(pollutant_hint, CONTAMINANTES_CANON) or pollutant_hint\n",
        "    for col in estacion_cols:\n",
        "        # station from column header (ex: 'SE' -> SURESTE)\n",
        "        station_guess = sheet_to_station.get(norm_text(col), None) or best_match_token(col, ESTACIONES_CANON)\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': df[date_col].values,\n",
        "            'station': [station_guess]*len(df),\n",
        "            'pollutant': [pollutant_guess]*len(df),\n",
        "            'value': df[col].values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = col\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# 2) DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx\n",
        "# - cada hoja = estacion, primera columna = fecha/hora, columnas = contaminantes\n",
        "def parse_2020_2021_todas(path, sheet):\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=0, engine='openpyxl')\n",
        "    df = drop_b_columns(df)\n",
        "    # primera columna = fecha\n",
        "    first = df.columns[0]\n",
        "    df = df.rename(columns={first: 'datetime'})\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce', dayfirst=True)\n",
        "    # columnas contaminantes: todas menos datetime\n",
        "    pollutant_cols = [c for c in df.columns if c != 'datetime']\n",
        "    rows = []\n",
        "    for col in pollutant_cols:\n",
        "        pollutant_guess = best_match_token(col, CONTAMINANTES_CANON)\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': df['datetime'].values,\n",
        "            'station': [sheet_to_station.get(sheet.strip().upper(), sheet.strip().upper())]*len(df),\n",
        "            'pollutant': [pollutant_guess]*len(df),\n",
        "            'value': df[col].values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = col\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# 3) DATOS HISTÓRICOS 2021_Contaminantes\n",
        "# - todo en una hoja; primer renglón = estación, segundo = contaminante; primera columna fecha, segunda hora\n",
        "def parse_2021_contaminantes(path, sheet):\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=None, engine='openpyxl')\n",
        "    # assume row0 = station headers, row1 = pollutant headers, data from row2\n",
        "    header0 = df.iloc[0].astype(str).tolist()\n",
        "    header1 = df.iloc[1].astype(str).tolist()\n",
        "    # construct multiindex\n",
        "    cols = []\n",
        "    for a,b in zip(header0, header1):\n",
        "        cols.append((norm_text(a), norm_text(b)))\n",
        "    data = df.iloc[2:].copy().reset_index(drop=True)\n",
        "    data.columns = pd.MultiIndex.from_tuples(cols)\n",
        "    # combine first two columns into datetime if separated\n",
        "    # find any column tuple that contains 'DATE' or 'FECHA' or 'HORA'\n",
        "    # We'll attempt to get index values as datetime from the first column that converts well\n",
        "    datetime_col = None\n",
        "    for col in data.columns:\n",
        "        a,b = col\n",
        "        if any(k in a for k in ['FECHA','DATE','DATETIME']) or any(k in b for k in ['FECHA','DATE','DATETIME']):\n",
        "            datetime_col = col\n",
        "            break\n",
        "    if datetime_col is None:\n",
        "        # try first column\n",
        "        datetime_col = data.columns[0]\n",
        "    # rename datetime col to ('DATETIME','')\n",
        "    data = data.rename(columns={datetime_col: ('DATETIME','')})\n",
        "    try:\n",
        "        data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')], errors='coerce', dayfirst=True)\n",
        "    except Exception:\n",
        "        data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')].astype(str), errors='coerce', dayfirst=True)\n",
        "    # now melt using other columns\n",
        "    rows = []\n",
        "    for col in data.columns:\n",
        "        if col == ('DATETIME',''):\n",
        "            continue\n",
        "        est_raw, poll_raw = col\n",
        "        station_guess = best_match_token(est_raw, ESTACIONES_CANON) or sheet_to_station.get(est_raw)\n",
        "        pollutant_guess = best_match_token(poll_raw, CONTAMINANTES_CANON)\n",
        "        serie = data[col]\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': data[('DATETIME','')].values,\n",
        "            'station': [station_guess]*len(serie),\n",
        "            'pollutant': [pollutant_guess]*len(serie),\n",
        "            'value': serie.values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = f\"{est_raw}__{poll_raw}\"\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# 4) DATOS HISTÓRICOS 2022_2023_TODAS ESTACIONES\n",
        "# - cada hoja = estacion; primera columna = fecha y hora; columnas = contaminantes\n",
        "def parse_2022_2023_todas(path, sheet):\n",
        "    # same logic as 2020_2021_todas\n",
        "    return parse_2020_2021_todas(path, sheet)\n",
        "\n",
        "# 5) DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM\n",
        "# - todo en una hoja; first row = station, second = pollutant, third = metric; first col = date, second = time\n",
        "def parse_2023_2024_itesm(path, sheet):\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=None, engine='openpyxl')\n",
        "    # header rows 0,1,2 -> use 0 and 1 for station/pollutant\n",
        "    header0 = df.iloc[0].astype(str).tolist()\n",
        "    header1 = df.iloc[1].astype(str).tolist()\n",
        "    header2 = df.iloc[2].astype(str).tolist()  # metrics (ignore)\n",
        "    cols = []\n",
        "    for a,b in zip(header0, header1):\n",
        "        cols.append((norm_text(a), norm_text(b)))\n",
        "    data = df.iloc[3:].copy().reset_index(drop=True)\n",
        "    data.columns = pd.MultiIndex.from_tuples(cols)\n",
        "    # detect datetime column similar to parse_2021\n",
        "    datetime_col = None\n",
        "    for col in data.columns:\n",
        "        a,b = col\n",
        "        if any(k in a for k in ['FECHA','DATE','DATETIME']) or any(k in b for k in ['FECHA','DATE','DATETIME']):\n",
        "            datetime_col = col\n",
        "            break\n",
        "    if datetime_col is None:\n",
        "        datetime_col = data.columns[0]\n",
        "    data = data.rename(columns={datetime_col: ('DATETIME','')})\n",
        "    try:\n",
        "        data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')], errors='coerce', dayfirst=True)\n",
        "    except Exception:\n",
        "        data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')].astype(str), errors='coerce', dayfirst=True)\n",
        "    rows = []\n",
        "    for col in data.columns:\n",
        "        if col == ('DATETIME',''):\n",
        "            continue\n",
        "        est_raw, poll_raw = col\n",
        "        station_guess = best_match_token(est_raw, ESTACIONES_CANON) or sheet_to_station.get(norm_text(est_raw))\n",
        "        pollutant_guess = best_match_token(poll_raw, CONTAMINANTES_CANON)\n",
        "        serie = data[col]\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': data[('DATETIME','')].values,\n",
        "            'station': [station_guess]*len(serie),\n",
        "            'pollutant': [pollutant_guess]*len(serie),\n",
        "            'value': serie.values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = f\"{est_raw}__{poll_raw}\"\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# 6) DATOS HISTÓRICOS 2024_TODAS ESTACIONES\n",
        "# - cada hoja = estación con abbreviations in sheet name (SE, NE, CE, etc)\n",
        "# - first column = date/time, first row = contaminant (units in parens ignore)\n",
        "def parse_2024_todas(path, sheet):\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=0, engine='openpyxl')\n",
        "    df = drop_b_columns(df)\n",
        "    # header includes contaminant names maybe with units \"(ug/m3)\" -> remove parenthesis\n",
        "    clean_cols = []\n",
        "    for c in df.columns:\n",
        "        cc = re.sub(r'\\(.*\\)', '', str(c))\n",
        "        clean_cols.append(cc)\n",
        "    df.columns = clean_cols\n",
        "    first = df.columns[0]\n",
        "    df = df.rename(columns={first: 'datetime'})\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce', dayfirst=True)\n",
        "    pollutant_cols = [c for c in df.columns if c != 'datetime']\n",
        "    rows = []\n",
        "    # station from sheet name\n",
        "    station_name = sheet_to_station.get(sheet.strip().upper(), sheet.strip().upper())\n",
        "    for col in pollutant_cols:\n",
        "        pollutant_guess = best_match_token(col, CONTAMINANTES_CANON)\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': df['datetime'].values,\n",
        "            'station': [station_name]*len(df),\n",
        "            'pollutant': [pollutant_guess]*len(df),\n",
        "            'value': df[col].values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = col\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# 7) DATOS HISTÓRICOS 2025_TODAS ESTACIONES\n",
        "# - same as 2024 but second row is metric (ignore)\n",
        "def parse_2025_todas(path, sheet):\n",
        "    df = pd.read_excel(path, sheet_name=sheet, header=0, engine='openpyxl')\n",
        "    df = drop_b_columns(df)\n",
        "    # if second row is metric, sometimes pandas read it as data. If the second row contains only units, drop it by checking datatypes.\n",
        "    # If the second row values are non-numeric for most pollutant columns, drop that row before melting.\n",
        "    # Let's inspect second row: if >50% of pollutant columns are non-numeric, drop row 0 (units)\n",
        "    # We will create a copy and attempt to coerce numeric on row 0 for pollutant columns\n",
        "    pollutant_cols = [c for c in df.columns[1:]] if len(df.columns) > 1 else []\n",
        "    if pollutant_cols:\n",
        "        nonnum_count = 0\n",
        "        for c in pollutant_cols:\n",
        "            try:\n",
        "                float(str(df.iloc[0][c]))\n",
        "            except Exception:\n",
        "                nonnum_count += 1\n",
        "        if nonnum_count > 0.5 * len(pollutant_cols):\n",
        "            # drop first row (units row)\n",
        "            df = df.iloc[1:].reset_index(drop=True)\n",
        "    # now same logic as parse_2024\n",
        "    clean_cols = []\n",
        "    for c in df.columns:\n",
        "        cc = re.sub(r'\\(.*\\)', '', str(c))\n",
        "        clean_cols.append(cc)\n",
        "    df.columns = clean_cols\n",
        "    first = df.columns[0]\n",
        "    df = df.rename(columns={first: 'datetime'})\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce', dayfirst=True)\n",
        "    station_name = sheet_to_station.get(sheet.strip().upper(), sheet.strip().upper())\n",
        "    rows = []\n",
        "    pollutant_cols = [c for c in df.columns if c != 'datetime']\n",
        "    for col in pollutant_cols:\n",
        "        pollutant_guess = best_match_token(col, CONTAMINANTES_CANON)\n",
        "        temp = pd.DataFrame({\n",
        "            'datetime': df['datetime'].values,\n",
        "            'station': [station_name]*len(df),\n",
        "            'pollutant': [pollutant_guess]*len(df),\n",
        "            'value': df[col].values\n",
        "        })\n",
        "        temp['source_sheet'] = sheet\n",
        "        temp['origin_col'] = col\n",
        "        rows.append(temp)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['datetime','station','pollutant','value','source_sheet','origin_col'])\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "# ------------------\n",
        "# Dispatcher que detecta según el nombre del archivo qué parser aplicar\n",
        "# ------------------\n",
        "def procesar_hoja_por_archivo(path, sheet, filename):\n",
        "    fname = Path(filename).name.upper()\n",
        "    if '2020_CONTAMINANTE' in fname:\n",
        "        return parse_2020_contaminante(path, sheet)\n",
        "    if '2020_2021' in fname:\n",
        "        return parse_2020_2021_todas(path, sheet)\n",
        "    if '2021_CONTAMINANTE' in fname:\n",
        "        # this file had a single sheet with double header\n",
        "        return parse_2021_contaminantes(path, sheet)\n",
        "    if '2022_2023' in fname:\n",
        "        return parse_2022_2023_todas(path, sheet)\n",
        "    if '2023_2024' in fname or 'ITESM' in fname:\n",
        "        return parse_2023_2024_itesm(path, sheet)\n",
        "    if '2024_TODAS' in fname:\n",
        "        return parse_2024_todas(path, sheet)\n",
        "    if '2025_TODAS' in fname:\n",
        "        return parse_2025_todas(path, sheet)\n",
        "    # fallback: try generic wide parse\n",
        "    try:\n",
        "        df0 = pd.read_excel(path, sheet_name=sheet, header=0, engine='openpyxl')\n",
        "        return parse_wide_sheet_from_df(df0, sheet)\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# ------------------\n",
        "# Build master function\n",
        "# ------------------\n",
        "def build_master_from_files(archivos, save_parquet=True, out_prefix=\"master_unificado_mapped\"):\n",
        "    partes = []\n",
        "    for archivo in archivos:\n",
        "        archivo_path = str(archivo)\n",
        "        print(f\"\\nProcesando archivo: {archivo_path}\")\n",
        "        if not os.path.exists(archivo_path):\n",
        "            print(\"  -> Archivo no encontrado, lo salto.\")\n",
        "            continue\n",
        "        try:\n",
        "            xls = pd.ExcelFile(archivo_path, engine='openpyxl')\n",
        "        except Exception as e:\n",
        "            print(\"  -> ERROR leyendo archivo:\", e)\n",
        "            continue\n",
        "        for hoja in xls.sheet_names:\n",
        "            print(f\"  Hoja: {hoja}\")\n",
        "            try:\n",
        "                parsed = procesar_hoja_por_archivo(archivo_path, hoja, archivo_path)\n",
        "                if parsed is None or parsed.empty:\n",
        "                    print(\"    -> No se extrajeron datos de esta hoja.\")\n",
        "                    continue\n",
        "                # Convert types\n",
        "                parsed['datetime'] = pd.to_datetime(parsed['datetime'], errors='coerce')\n",
        "                parsed['value'] = pd.to_numeric(parsed['value'], errors='coerce')\n",
        "                parsed['source_file'] = Path(archivo_path).name\n",
        "                parsed['source_sheet'] = hoja\n",
        "                # if station is missing, try to use sheet name hint (applies for many cases)\n",
        "                parsed['station'] = parsed['station'].fillna(parsed['source_sheet'].apply(lambda s: sheet_to_station.get(s.strip().upper())))\n",
        "                # keep rows (including NaN values in 'value') as you requested\n",
        "                partes.append(parsed)\n",
        "                print(f\"    -> filas extraídas: {len(parsed)}\")\n",
        "            except Exception as e:\n",
        "                print(\"    -> ERROR procesando hoja:\", e)\n",
        "                continue\n",
        "    if not partes:\n",
        "        print(\"No se extrajo ninguna parte.\")\n",
        "        return pd.DataFrame()\n",
        "    master = pd.concat(partes, ignore_index=True)\n",
        "    # Map stations more robustly (normalize abbreviations and tokens)\n",
        "    def map_station(x):\n",
        "        if pd.isna(x):\n",
        "            return None\n",
        "        s = str(x).strip()\n",
        "        su = s.upper()\n",
        "        # direct mapping from sheet_to_station keys\n",
        "        if su in sheet_to_station:\n",
        "            return sheet_to_station[su]\n",
        "        # direct canonical match or token match\n",
        "        bm = best_match_token(su, ESTACIONES_CANON)\n",
        "        if bm:\n",
        "            return bm\n",
        "        # remove trailing digits / punctuation and try again\n",
        "        s2 = re.sub(r'[\\d\\._\\-]','', su).strip()\n",
        "        bm2 = best_match_token(s2, ESTACIONES_CANON)\n",
        "        if bm2:\n",
        "            return bm2\n",
        "        return s  # fallback: keep original textual value\n",
        "    master['station_original'] = master['station']\n",
        "    master['station'] = master['station'].apply(map_station)\n",
        "    # map pollutant tokens\n",
        "    master['pollutant_original'] = master['pollutant']\n",
        "    master['pollutant'] = master['pollutant'].apply(lambda x: best_match_token(x, CONTAMINANTES_CANON) if pd.notna(x) else x)\n",
        "    # remove completely empty rows\n",
        "    master = master[~(master['station'].isna() & master['pollutant'].isna() & master['value'].isna() & master['datetime'].isna())]\n",
        "    # drop exact duplicate rows if any (keep last)\n",
        "    master = master.drop_duplicates(subset=['datetime','station','pollutant','value','source_file','source_sheet'], keep='last')\n",
        "    master = master.sort_values(['datetime','station','pollutant']).reset_index(drop=True)\n",
        "    # Save\n",
        "    if save_parquet:\n",
        "        outp = f\"/content/{out_prefix}.parquet\"\n",
        "        try:\n",
        "            master.to_parquet(outp, index=False)\n",
        "            print(f\"\\nMaster guardado en: {outp}\")\n",
        "        except Exception as e:\n",
        "            print(\"No se pudo guardar parquet:\", e)\n",
        "    return master\n",
        "\n",
        "def pivot_to_wide(master_df, fillna=False, out_path=\"/content/master_unificado_mapped_wide.xlsx\"):\n",
        "    df = master_df.copy()\n",
        "    df = df.dropna(subset=['datetime'], how='all')\n",
        "    df['colname'] = df['station'].astype(str) + \"_\" + df['pollutant'].astype(str)\n",
        "    wide = df.pivot_table(index='datetime', columns='colname', values='value', aggfunc='first')\n",
        "    wide = wide.sort_index(axis=1)\n",
        "    if fillna is not False:\n",
        "        wide = wide.fillna(fillna)\n",
        "    try:\n",
        "        wide.to_excel(out_path)\n",
        "        print(f\"Versión ancha guardada en: {out_path}\")\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo guardar versión ancha:\", e)\n",
        "    return wide\n",
        "\n",
        "# ------------------\n",
        "# EJECUCIÓN - pega los paths exactos de tus 7 archivos\n",
        "# ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    archivos = [\n",
        "        # actualiza rutas si trabajas en Colab (monta drive y usa /content/drive/MyDrive/...)\n",
        "        \"/content/DATOS HISTÓRICOS 2020_Contaminante.xlsx\",\n",
        "        \"/content/DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx\",\n",
        "        \"/content/DATOS HISTÓRICOS 2021_Contaminante.xlsx\",\n",
        "        \"/content/DATOS HISTÓRICOS 2022_2023_TODAS ESTACIONES.xlsx\",\n",
        "        \"/content/DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM.xlsx\",  # el que subiste\n",
        "        \"/content/DATOS HISTÓRICOS 2024_TODAS ESTACIONES.xlsx\",\n",
        "        \"/content/DATOS HISTÓRICOS 2025_TODAS ESTACIONES.xlsx\"\n",
        "    ]\n",
        "\n",
        "    master = build_master_from_files(archivos, save_parquet=True, out_prefix=\"master_unificado_mapped\")\n",
        "    print(\"\\nMaster final - primeras filas:\")\n",
        "    print(master.head(10))\n",
        "\n",
        "    # crear versión ancha y guardar\n",
        "    wide = pivot_to_wide(master, fillna=False, out_path=\"/content/master_unificado_mapped_wide.xlsx\")\n",
        "    print(\"Listo. Revisa los archivos en /content/ o en la ruta que hayas elegido para guardar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2JDlHddi9GO",
        "outputId": "7c4804ca-ae01-42d4-bb24-0f627326d45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2020_Contaminante.xlsx\n",
            "  Hoja: PM10\n",
            "    -> filas extraídas: 122850\n",
            "  Hoja: PM2.5\n",
            "    -> filas extraídas: 122066\n",
            "  Hoja: O3\n",
            "    -> filas extraídas: 122850\n",
            "  Hoja: NO\n",
            "    -> filas extraídas: 122850\n",
            "  Hoja: NO2\n",
            "    -> filas extraídas: 122836\n",
            "  Hoja: NOx\n",
            "    -> filas extraídas: 122850\n",
            "  Hoja: CO\n",
            "    -> filas extraídas: 122668\n",
            "  Hoja: SO2\n",
            "    -> filas extraídas: 122836\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx\n",
            "  Hoja: SURESTE\n",
            "    -> filas extraídas: 263070\n",
            "  Hoja: NORESTE\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: CENTRO\n",
            "    -> filas extraídas: 263040\n",
            "  Hoja: NOROESTE\n",
            "    -> filas extraídas: 263055\n",
            "  Hoja: SUROESTE\n",
            "    -> filas extraídas: 263055\n",
            "  Hoja: NOROESTE2\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: NORTE\n",
            "    -> filas extraídas: 263040\n",
            "  Hoja: SUROESTE2\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: SURESTE2\n",
            "    -> filas extraídas: 263040\n",
            "  Hoja: SURESTE3\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: SUR\n",
            "    -> filas extraídas: 263010\n",
            "  Hoja: NORTE2\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: NORESTE2\n",
            "    -> filas extraídas: 263025\n",
            "  Hoja: NORESTE3\n",
            "    -> filas extraídas: 263130\n",
            "  Hoja: NOROESTE3\n",
            "    -> No se extrajeron datos de esta hoja.\n",
            "  Hoja: CATÁLOGO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1929529637.py:166: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce', dayfirst=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    -> filas extraídas: 30\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2021_Contaminante.xlsx\n",
            "  Hoja: Param_horarios_Estaciones\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1929529637.py:213: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
            "  data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')], errors='coerce', dayfirst=True)\n",
            "/tmp/ipython-input-1929529637.py:215: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
            "  data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')].astype(str), errors='coerce', dayfirst=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    -> ERROR procesando hoja: 'DATETIME'\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2022_2023_TODAS ESTACIONES.xlsx\n",
            "  Hoja: SURESTE\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NORESTE\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: CENTRO\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NOROESTE\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: SUROESTE\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NOROESTE2\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NORTE\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: SUROESTE2\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: SURESTE2\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: SURESTE3\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: SUR\n",
            "    -> filas extraídas: 213810\n",
            "  Hoja: NORTE2\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NORESTE2\n",
            "    -> filas extraídas: 213825\n",
            "  Hoja: NORESTE3\n",
            "    -> filas extraídas: 213810\n",
            "  Hoja: NOROESTE3\n",
            "    -> filas extraídas: 93555\n",
            "  Hoja: CATÁLOGO\n",
            "    -> filas extraídas: 30\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM.xlsx\n",
            "  Hoja: Param_horarios_Estaciones\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1929529637.py:166: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce', dayfirst=True)\n",
            "/tmp/ipython-input-1929529637.py:268: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
            "  data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')], errors='coerce', dayfirst=True)\n",
            "/tmp/ipython-input-1929529637.py:270: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
            "  data[('DATETIME','')] = pd.to_datetime(data[('DATETIME','')].astype(str), errors='coerce', dayfirst=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    -> ERROR procesando hoja: 'DATETIME'\n",
            "  Hoja: Hoja2\n",
            "    -> ERROR procesando hoja: single positional indexer is out-of-bounds\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2024_TODAS ESTACIONES.xlsx\n",
            "  Hoja: SE\n",
            "    -> filas extraídas: 131760\n",
            "  Hoja: CE\n",
            "    -> filas extraídas: 131760\n",
            "  Hoja: SO\n",
            "    -> filas extraídas: 131760\n",
            "  Hoja: NE2\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: SE2\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: SE3\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: NE\n",
            "    -> filas extraídas: 131760\n",
            "  Hoja: NO\n",
            "    -> filas extraídas: 131745\n",
            "  Hoja: NO2\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: NTE\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: NTE2\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: SO2\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: SUR\n",
            "    -> filas extraídas: 131730\n",
            "  Hoja: NO3\n",
            "    -> filas extraídas: 131760\n",
            "  Hoja: NE3\n",
            "    -> filas extraídas: 131730\n",
            "\n",
            "Procesando archivo: /content/DATOS HISTÓRICOS 2025_TODAS ESTACIONES.xlsx\n",
            "  Hoja: SE\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: CE\n",
            "    -> filas extraídas: 65160\n",
            "  Hoja: SO\n",
            "    -> filas extraídas: 63030\n",
            "  Hoja: NE2\n",
            "    -> filas extraídas: 65130\n",
            "  Hoja: SE2\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: SE3\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: NE\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: NO\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: NO2\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: NTE\n",
            "    -> filas extraídas: 65160\n",
            "  Hoja: NTE2\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: SO2\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: SUR\n",
            "    -> filas extraídas: 65145\n",
            "  Hoja: NO3\n",
            "    -> filas extraídas: 64800\n",
            "  Hoja: NE3\n",
            "    -> filas extraídas: 64800\n",
            "\n",
            "Master guardado en: /content/master_unificado_mapped.parquet\n",
            "\n",
            "Master final - primeras filas:\n",
            "    datetime station pollutant  value source_sheet origin_col  \\\n",
            "0 2020-01-01  CENTRO        CO    NaN           CO         CE   \n",
            "1 2020-01-01  CENTRO        CO    NaN       CENTRO         CO   \n",
            "2 2020-01-01  CENTRO        NO    NaN           NO         CE   \n",
            "3 2020-01-01  CENTRO        NO    NaN       CENTRO         NO   \n",
            "4 2020-01-01  CENTRO       NO2    NaN          NO2         CE   \n",
            "5 2020-01-01  CENTRO       NO2    NaN       CENTRO        NO2   \n",
            "6 2020-01-01  CENTRO       NOX    NaN          NOx         CE   \n",
            "7 2020-01-01  CENTRO       NOX    NaN       CENTRO        NOX   \n",
            "8 2020-01-01  CENTRO        O3    NaN           O3         CE   \n",
            "9 2020-01-01  CENTRO        O3    NaN       CENTRO         O3   \n",
            "\n",
            "                                         source_file station_original  \\\n",
            "0           DATOS HISTÓRICOS 2020_Contaminante.xlsx           CENTRO   \n",
            "1  DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx           CENTRO   \n",
            "2           DATOS HISTÓRICOS 2020_Contaminante.xlsx           CENTRO   \n",
            "3  DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx           CENTRO   \n",
            "4           DATOS HISTÓRICOS 2020_Contaminante.xlsx           CENTRO   \n",
            "5  DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx           CENTRO   \n",
            "6           DATOS HISTÓRICOS 2020_Contaminante.xlsx           CENTRO   \n",
            "7  DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx           CENTRO   \n",
            "8           DATOS HISTÓRICOS 2020_Contaminante.xlsx           CENTRO   \n",
            "9  DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx           CENTRO   \n",
            "\n",
            "  pollutant_original  \n",
            "0                 CO  \n",
            "1                 CO  \n",
            "2                 NO  \n",
            "3                 NO  \n",
            "4                NO2  \n",
            "5                NO2  \n",
            "6                NOX  \n",
            "7                NOX  \n",
            "8                 O3  \n",
            "9                 O3  \n",
            "Versión ancha guardada en: /content/master_unificado_mapped_wide.xlsx\n",
            "Listo. Revisa los archivos en /content/ o en la ruta que hayas elegido para guardar.\n"
          ]
        }
      ]
    }
  ]
}